{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d8470aa5f20835",
   "metadata": {},
   "source": [
    "# Bedrock Console Demo \n",
    "\n",
    "Perhaps you've opened this lab and immediately thought \n",
    "> git, command lines, APIs...huh?\n",
    " \n",
    "and are looking for a less technically involved workshop. Perhaps you have limited time but you're still interested in how large machine learning models work. Thankfully, Amazon Bedrock provides a console experience for you to interact with foundation models (FMs) like Amazon's Titan family or Anthropic's Claude family.\n",
    "\n",
    "In this notebook, we'll walk you through the steps included in the individual modules for interacting with the models. You'll enter a prompt in the Bedrock playground, observing the model's response and determining the impact the inference parameters have on the the generated text. We'll follow along with the prompts that are included in the modules, pulling out the snippets where we interact with the models, and focus more on working in the console.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook assumes you have access to the console in an AWS account for which you have Amazon Bedrock permissions. Please work with your AWS administrator to obtain appropriate access. You won't need any experience in large language models to complete this notebook, but having a general awareness of generative artificial intelligence can be helpful.\n",
    "\n",
    "\n",
    "## Getting started\n",
    "\n",
    "First, navigate in the console to the Bedrock Service page, found at this link:\n",
    "\n",
    "[https://us-east-1.console.aws.amazon.com/bedrock/home](https://us-east-1.console.aws.amazon.com/bedrock/home)\n",
    "\n",
    "Please note that the link above will send you to the console in the US East 1 (Northern Virginia) region. If you operate in a different region, please be sure to select the appropriate region from the top right corner of your console window, as seen in the picture below.\n",
    "\n",
    "![](./imgs/console-region-selection.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acd65cc86f233d0",
   "metadata": {},
   "source": [
    "Once you've logged into the console and confirmed you're in the correct region, you can navigate to the playgrounds by finding them in the left hand menu, which can be expanded using the menu button in the top left corner.\n",
    "\n",
    "![](./imgs/bedrock-menu.png)\n",
    "\n",
    "There are three playgrounds within the Bedrock console:\n",
    "- Chat: interactive near-realtime chat with a generative AI, streamed in the browser\n",
    "- Text: longer form textual analysis and context driven Q&A\n",
    "- Image: prompt based image generation\n",
    "\n",
    "In this lab, we'll be working with the Chat and Text playgrounds, beginning with the Chat playground. Select it from the menu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88e2f71f8d2482",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a727ba898f9fe4e1",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Amazon Bedrock supports FMs from several organizations specializing in the development of artificial intelligence, such as Anthropic, Cohere, AI21, and Stability AI, along with Amazon's own Titan family of large language models. For this lab, we'll be using Amazon's Titan models.\n",
    "\n",
    "1. Choose Amazon from the \"Select model category\" drop down menu.\n",
    "2. Choose Titan Text G1 - Express from the \"Select model\" drop down menu.\n",
    "\n",
    "This tells the console that you'd like to send your prompts to the `Titan Text G1 - Express` endpoint. Once you've configured your Chat playground and are ready to begin chatting, your window should look like this:\n",
    "\n",
    "![](imgs/chat-window.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0bd54b75922817",
   "metadata": {},
   "source": [
    "## 01_Generation\n",
    "\n",
    "Start out by asking some questions of the model. \n",
    "\n",
    "For example, I asked a question about requirements for selling securities in the United States. When I hit run, I received this output from the model:\n",
    "\n",
    "![](imgs/series7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23049b56deabbbd",
   "metadata": {},
   "source": [
    "You can specify other requests or questions. For example, try prompting the model with:\n",
    "- Please provide a list of ingredients for making apple pie.\n",
    "- What is Number Six's famous line in the first episode of The Prisoner?\n",
    "- Please show me a Python 3 implementation of dimensionality reduction via principal component analysis that does not include the sklearn Python module.\n",
    "\n",
    "Try a few prompts of your own choosing. Keep in mind though--many of these models have a strong command of syntax and vocabulary, but may not have deep expertise in a given domain. As we'll see later, it can be helpful to provide instructions to the model about what to do when it doesn't know an answer. That way, we avoid [model hallucinations](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)), as you may already be aware of.\n",
    "\n",
    "Another way to customize the model's response is by updating the inference parameters associated with the request. These parameters are described in the next section and tweak the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa6913993210aa",
   "metadata": {},
   "source": [
    "## Common inference parameter definitions\n",
    "\n",
    "### Randomness and Diversity\n",
    "\n",
    "Foundation models support the following parameters to control randomness and diversity in the response.\n",
    "\n",
    "##### Temperature\n",
    "Large language models use probability construct the words in a sequence. For any given next word, there is a probability distribution of options for the next word in the sequence. When you set the temperature closer to zero, the model tends to select the higher-probability words. When you set the temperature further away from zero, the model may select a lower-probability word.\n",
    "\n",
    "In technical terms, the temperature modulates the probability density function for the next tokens, implementing the temperature sampling technique. This parameter can deepen or flatten the density function curve. A lower value results in a steeper curve with more deterministic responses, and a higher value results in a flatter curve with more random responses.\n",
    "\n",
    "##### Top-K \n",
    "Temperature defines the probability distribution of potential words, and Top K defines the cut off where the model no longer selects the words. For example, if K=50, the model selects from 50 of the most probable words that could be next in a given sequence. This reduces the probability that an unusual word gets selected next in a sequence. In technical terms, Top K is the number of the highest-probability vocabulary tokens to keep for Top-K-filtering - This limits the distribution of probable tokens, so the model chooses one of the highest probability tokens.\n",
    "\n",
    "##### Top P\n",
    "Top P defines a cut off based on the sum of probabilities of the potential choices. If you set Top P below 1.0, the model considers the most probable options and ignores less probable ones. Top P is similar to Top K, but instead of capping the number of choices, it caps choices based on the sum of their probabilities. For the example prompt \"I hear the hoof beats of ,\" you may want the model to provide \"horses,\" \"zebras\" or \"unicorns\" as the next word. If you set the temperature to its maximum, without capping Top K or Top P, you increase the probability of getting unusual results such as \"unicorns.\" If you set the temperature to 0, you increase the probability of \"horses.\" If you set a high temperature and set Top K or Top P to the maximum, you increase the probability of \"horses\" or \"zebras,\" and decrease the probability of \"unicorns.\"\n",
    "\n",
    "\n",
    "\n",
    "### Length\n",
    "\n",
    "The following parameters control the length of the generated response.\n",
    "\n",
    "##### Response length\n",
    "Configures the minimum and maximum number of tokens to use in the generated response.\n",
    "\n",
    "##### Length penalty\n",
    "Length penalty optimizes the model to be more concise in its output by penalizing longer responses. Length penalty differs from response length as the response length is a hard cut off for the minimum or maximum response length.\n",
    "\n",
    "In technical terms, the length penalty penalizes the model exponentially for lengthy responses. 0.0 means no penalty. Set a value less than 0.0 for the model to generate longer sequences, or set a value greater than 0.0 for the model to produce shorter sequences.\n",
    "\n",
    "### Repetitions\n",
    "\n",
    "The following parameters help control repetition in the generated response.\n",
    "\n",
    "##### Repetition penalty (presence penalty) \n",
    "Prevents repetitions of the same words (tokens) in responses. 1.0 means no penalty. Greater than 1.0 decreases repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7589c65ea4ebe",
   "metadata": {},
   "source": [
    "In the Chat playground, you'll see `Default` in the top right of the prompt box, reflecting that the prompt was sent to the Bedrock API using the default inference parameters.\n",
    "\n",
    "![](imgs/carryforward.png)\n",
    "\n",
    "Near the `Run` button, you'll see a link that says \"Update inference configurations.\" Pressing that button opens up the window pictured below.\n",
    "\n",
    "![](imgs/inference-parameters.png)\n",
    "\n",
    "For example, the length of the response can be controlled using the length parameter. By setting it to `1`, you'll limit the response length to a single token--which will likely be part of the first word in the response. You'll also notice that the inference parameters used for a given inference are captured in the prompt box.\n",
    "\n",
    "![](imgs/response-length.png)\n",
    "\n",
    "Setting the length parameter to a significantly higher number (perhaps all the way up to `4096`) will allow for longer answers. Keep in mind that a longer answer is not necessarily a higher quality answer, but each token does create an expense. In other words, as described above, choosing a response length that is both concise and appropriate in its level of detail is paramount for the cost efficiency of your generative applications.\n",
    "\n",
    "Feel free to play around with the other parameters to see how they may impact the responses provided by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335fde8796e1e2d3",
   "metadata": {},
   "source": [
    "## 02_Summarization\n",
    "\n",
    "Large language models like Titan can also summarize text. For this example, switch to the Text playground from the left hand menu in the console.\n",
    "\n",
    "As before, select the Amazon model category and the Titan Text G1 - Express model. Once you've selected the model, your console page should appear as pictured below.\n",
    "\n",
    "![](imgs/text-playground.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0564a6593fec59",
   "metadata": {},
   "source": [
    "### Short Text Summarization\n",
    "We'll follow along with the prompts from the Summarization module here. For the first, we're going to ask the model to summarize the announcement for Amazon Bedrock, which is copied below:\n",
    "\n",
    "\n",
    "> Please provide a summary of the following text. Do not add any information that is not mentioned in the text below.\n",
    ">\n",
    ">AWS took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, a new service that makes FMs from AI21 Labs, Anthropic Stability AI, and Amazon accessible via an API. \n",
    "> \n",
    "> Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, democratizing access for all builders. Bedrock will offer the ability to access a range of powerful FMs for text and images—including Amazons Titan FMs, which consist of two new LLMs we’re also announcing today—through a scalable, reliable, and secure AWS managed service. \n",
    "> \n",
    "> With Bedrock’s serverless experience, customers can easily find the right model for what they’re trying to get done, get started quickly, privately customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale).\n",
    "\n",
    "You can copy the text from the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc98714d7ee5310",
   "metadata": {},
   "source": [
    "``` \n",
    "Please provide a summary of the following text. Do not add any information that is not mentioned in the text below.\n",
    "\n",
    "AWS took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, a new service that makes FMs from AI21 Labs, Anthropic Stability AI, and Amazon accessible via an API.\n",
    "Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, democratizing access for all builders. Bedrock will offer the ability to access a range of powerful FMs for text and images—including Amazons Titan FMs, which consist of two new LLMs we’re also announcing today—through a scalable, reliable, and secure AWS managed service. With Bedrock’s serverless experience, customers can easily find the right model for what they’re trying to get done, get started quickly, privately customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale). \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa216bd1b9931c1d",
   "metadata": {},
   "source": [
    "Without changing the inference parameters, you can see that the model does a fairly decent job at summarizing the announcement, with its summary in green text immediately below the announcement text.\n",
    "\n",
    "![](imgs/short-summary.png)\n",
    "\n",
    "Feel free to tweak the parameters and re-run the prompt to see how it impacts the response. For example, increasing the temperature will change the specific details included in the summary. \n",
    "\n",
    "In the background, your console session is interacting with the `bedrock` and `bedrock-runtime` application programming interfaces (APIs). You can see the request being sent to the API by clicking the \"View API request\" button in the lower right corner of the Text playground window. The a truncated version of the previous example's API request is captured below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f6e81eb41018e",
   "metadata": {},
   "source": [
    "{\n",
    "  \"modelId\": \"amazon.titan-text-express-v1\",\n",
    "  \"contentType\": \"application/json\",\n",
    "  \"accept\": \"*/*\",\n",
    "  \"body\": \"{\\\"inputText\\\":\\\"Please provide a summary of the following text. Do not add any information...without managing any infrastructure.\\\",\\\"textGenerationConfig\\\":{\\\"maxTokenCount\\\":512,\\\"stopSequences\\\":[],\\\"temperature\\\":0,\\\"topP\\\":0.9}}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f07f3483b370c4",
   "metadata": {},
   "source": [
    "As you can see, the console has formatted your prompt along with your selected inference parameters and is prepared to send it to the `bedrock-runtime` API. If you choose to complete the remaining modules, you'll create these API requests manually. For now, just know that the code within the Console is creating and sending these requests on your behalf under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0888c2fe0155e6",
   "metadata": {},
   "source": [
    "### Long Text Summarization\n",
    "Large language models are also capable of summarizing longer text. In some cases, the text may be sufficiently long that it must be split up across multiple \"chunks\" that are then provided to the model. Within the console, we'll simply provide the entirety of the text we want summarized. For this example, we'll use Andy Jassy's 2022 Letter to Amazon Shareholders, the text of which can be found in this repository at [02_Summarization/letters/2022-letter.txt](02_Summarization/letters/2022-letter.txt).\n",
    "\n",
    "Before providing the text to the model, we'll need to provide it with instructions what to do. We'll add the following to the top of our prompt:\n",
    "\n",
    "\n",
    "> Please provide a summary for the following text. Use only the information below to create the summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a789a6d2cdf90",
   "metadata": {},
   "source": [
    "Now, open the letter mentioned above and copy all of the text onto your clipboard. Paste it beneath your prompt, and hit run.\n",
    "\n",
    "![](imgs/jeff-bezos.png)\n",
    "\n",
    "Wait...Jeff who? Wasn't his [second letter to the shareholders](http://media.corporate-ir.net/media_files/irol/97/97664/reports/Shareholderletter98.pdf) written back in 1998?\n",
    "\n",
    "As we can see, the model made a mistake here. Andy's letter to the shareholders doesn't mention his or Jeff's names--how did it come to the conclusion that Jeff was still CEO? Chances are, the data upon which the model was trained maintains such a strong association between the concepts of `Amazon`, `Jeff Bezos`, and `CEO` that his position in that role was assumed to still be accurate. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e56f8cdf0baada",
   "metadata": {},
   "source": [
    "Let's see if through prompt engineering we can correct that error by changing the prompt we provide the model. Change the prompt ahead of the text to say\n",
    "> Please use the following text and what you already know about Amazon to create a summary:\n",
    "\n",
    "You should notice that upon execution, the model now correctly ascribes the letter to the current Amazon CEO, Andy Jassy.\n",
    "\n",
    "![](imgs/andy-jassy.png)\n",
    "\n",
    "Manipulating the prompts given to the model is one of the most important methods you'll use to improve the responses that are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b539313bb2124",
   "metadata": {},
   "source": [
    "## 03_Question Answering\n",
    "\n",
    "As mentioned before, large language models are trained to emulate a strong understanding of natural language. They are _not_ trained to be experts in every particular field. Sometimes, you'll want to use a model to answer specific questions about a niche field. In order to accomplish this, we provide additional information to the model that it can use to address our prompts.\n",
    "\n",
    "This method relies on a technique called retrieval augmented generation (RAG), which combines the benefits of retrieval-based generation and attention-based generation. RAG uses a retrieval model to select relevant documents from a dataset and then uses an attention-based model to generate a summary or paraphrase of the selected content.\n",
    "\n",
    "For example, let's prompt the model to help us change a tire on our Audi A8. \n",
    "\n",
    "![](imgs/audi-spare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8445997488b6c18",
   "metadata": {},
   "source": [
    "While these instructions seem correct, the Audi A8 doesn't have a spare tire. Instead, it has a tire repair kit with which a driver can patch their tire. For most cars, the instructions provided are appropriate. In this particular case, however, the model is guessing incorrectly based on what it believe generally to be true about vehicle maintenance. \n",
    "\n",
    "Let's ask it to take a guess about something that doesn't exist--the Amazon Tirana:\n",
    "\n",
    "![](imgs/tirana.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d35f68dac05028",
   "metadata": {},
   "source": [
    "The model assumed that the imaginary Tirana is actually a bicycle, and provided general instructions about repairing a flat tire on a bicycle. The Tirana doesn't exist, so we want to make sure the model won't provide an incorrect answer if it doesn't know. To achieve that, we simply specify as much in the prompt:\n",
    "\n",
    "![](imgs/tirana-notsure.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75ee16f8b07cdc",
   "metadata": {},
   "source": [
    "Now, Titan admits that it doesn't know how to repair a flat tire on the Tirana, but still provides general steps (that in this case, happens to be almost correct). Let's take it one step further and revisit our stranded Audi. If we happen to have the owner's manual, we can provide it as context to the model so that it can be used by the model to provide a better answer. I've copied a (fictional) portion of the owner's manual below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fada1786a9af4",
   "metadata": {},
   "source": [
    "> Tires and tire pressure:\n",
    ">\n",
    "> Tires are made of black rubber and are mounted on the wheels of your vehicle. They provide the necessary grip for driving, cornering, and braking. Two important factors to consider are tire pressure and tire wear, as they can affect the performance and handling of your car.\n",
    ">\n",
    "> Where to find recommended tire pressure:\n",
    ">\n",
    "> You can find the recommended tire pressure specifications on the inflation label located on the driver's side B-pillar of your vehicle. Alternatively, you can refer to your vehicle's manual for this information. The recommended tire pressure may vary depending on the speed and the number of occupants or maximum load in the vehicle.\n",
    ">\n",
    "> Reinflating the tires:\n",
    ">\n",
    "> When checking tire pressure, it is important to do so when the tires are cold. This means allowing the vehicle to sit for at least three hours to ensure the tires are at the same temperature as the ambient temperature.\n",
    ">\n",
    ">To reinflate the tires:\n",
    ">\n",
    ">    Check the recommended tire pressure for your vehicle.\n",
    ">    Follow the instructions provided on the air pump and inflate the tire(s) to the correct pressure.\n",
    ">    In the center display of your vehicle, open the \"Car status\" app.\n",
    ">    Navigate to the \"Tire pressure\" tab.\n",
    ">    Press the \"Calibrate pressure\" option and confirm the action.\n",
    ">    Drive the car for a few minutes at a speed above 30 km/h to calibrate the tire pressure.\n",
    ">\n",
    "> Note: In some cases, it may be necessary to drive for more than 15 minutes to clear any warning symbols or messages related to tire pressure. If the warnings persist, allow the tires to cool down and repeat the above steps.\n",
    ">\n",
    ">Flat Tire:\n",
    ">\n",
    ">If you encounter a flat tire while driving, you can temporarily seal the puncture and reinflate the tire using a tire mobility kit. This kit is typically stored under the lining of the luggage area in your vehicle.\n",
    ">\n",
    ">Instructions for using the tire mobility kit:\n",
    ">\n",
    ">    Open the tailgate or trunk of your vehicle.\n",
    ">    Lift up the lining of the luggage area to access the tire mobility kit.\n",
    ">    Follow the instructions provided with the tire mobility kit to seal the puncture in the tire.\n",
    ">    After using the kit, make sure to securely put it back in its original location.\n",
    ">    Contact Audi or an appropriate service for assistance with disposing of and replacing the used sealant bottle.\n",
    ">\n",
    ">Please note that the tire mobility kit is a temporary solution and is designed to allow you to drive for a maximum of 10 minutes or 8 km (whichever comes first) at a maximum speed of 80 km/h. It is advisable to replace the punctured tire or have it repaired by a professional as soon as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d9658af576962c",
   "metadata": {},
   "source": [
    "You can find the content above at [03_QuestionAnswering/data/audi.txt](03_QuestionAnswering/data/audi.txt). We'll change our prompt to reference the content from the manual as context with which it can address your prompt, pulling out the specific information you need. Start by beginning your prompt with:\n",
    "\n",
    ">I'm going to provide an excerpt from the Audi owner's manual. Please use only this information to tell me how to repair a flat tire on my Audi A8.\n",
    "\n",
    "Beneath this, copy the text from the owner's manual and run the prompt. Now, you should see the steps extracted from the information you provided. \n",
    "\n",
    "![](imgs/audi-correct.png)\n",
    "\n",
    "By providing context that the model needs to address the prompt allows you to build from the model's general understanding of natural language and apply it to specific domains that are relevant to your use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ee284b7e41d0f",
   "metadata": {},
   "source": [
    "## Wrapping it all up\n",
    "\n",
    "In this lab session, we learned about interacting with Amazon Bedrock via the AWS Console. You saw the impact that inference parameters had on the responses provided by a model to a given a prompt. We leveraged RAG to find the specific and accurate answer to questions based large volumes of information we know might be relevant. \n",
    "\n",
    "Because Amazon Bedrock executes your prompts against models on your behalf, there's nothing for you to clean up in this lab. Simply log out of your console session and follow whatever steps are necessary to invalidate any temporary credentials used for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413106763e8d4f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
